{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OKVYQxFYyXb"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "gcbgTss4b_Iy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "494c9743-8df8-49db-f970-7c1cdc88f1e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crewai==0.76.9\n",
            "  Downloading crewai-0.76.9-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting crewai_tools==0.13.4\n",
            "  Downloading crewai_tools-0.13.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain_community==0.3.5\n",
            "  Downloading langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting appdirs>=1.4.4 (from crewai==0.76.9)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting auth0-python>=4.7.1 (from crewai==0.76.9)\n",
            "  Downloading auth0_python-4.7.2-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting chromadb>=0.4.24 (from crewai==0.76.9)\n",
            "  Downloading chromadb-0.5.18-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (8.1.7)\n",
            "Collecting instructor>=1.3.3 (from crewai==0.76.9)\n",
            "  Downloading instructor-1.6.4-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting json-repair>=0.25.2 (from crewai==0.76.9)\n",
            "  Downloading json_repair-0.30.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting jsonref>=1.1.0 (from crewai==0.76.9)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain>=0.2.16 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (0.3.7)\n",
            "Collecting litellm>=1.44.22 (from crewai==0.76.9)\n",
            "  Downloading litellm-1.52.9-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: openai>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (1.54.4)\n",
            "Requirement already satisfied: opentelemetry-api>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (1.28.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http>=1.22.0 (from crewai==0.76.9)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (1.28.1)\n",
            "Requirement already satisfied: pydantic>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (2.9.2)\n",
            "Collecting python-dotenv>=1.0.0 (from crewai==0.76.9)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting pyvis>=0.3.2 (from crewai==0.76.9)\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (2024.9.11)\n",
            "Collecting tomli-w>=1.1.0 (from crewai==0.76.9)\n",
            "  Downloading tomli_w-1.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: tomli>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from crewai==0.76.9) (2.1.0)\n",
            "Collecting uv>=0.4.25 (from crewai==0.76.9)\n",
            "  Downloading uv-0.5.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.13.4) (4.12.3)\n",
            "Collecting docker>=7.1.0 (from crewai_tools==0.13.4)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting docx2txt>=0.8 (from crewai_tools==0.13.4)\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting embedchain>=0.1.114 (from crewai_tools==0.13.4)\n",
            "  Downloading embedchain-0.1.125-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting lancedb>=0.5.4 (from crewai_tools==0.13.4)\n",
            "  Downloading lancedb-0.16.0-cp38-abi3-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting pyright>=1.1.350 (from crewai_tools==0.13.4)\n",
            "  Downloading pyright-1.1.389-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pytest>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.13.4) (8.3.3)\n",
            "Collecting pytube>=15.0.0 (from crewai_tools==0.13.4)\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from crewai_tools==0.13.4) (2.32.3)\n",
            "Collecting selenium>=4.18.1 (from crewai_tools==0.13.4)\n",
            "  Downloading selenium-4.26.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (6.0.2)\n",
            "Collecting SQLAlchemy<2.0.36,>=1.4 (from langchain_community==0.3.5)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (3.11.1)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community==0.3.5)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community==0.3.5)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (0.3.18)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community==0.3.5)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community==0.3.5) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.5) (4.0.3)\n",
            "Requirement already satisfied: cryptography<44.0.0,>=43.0.1 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai==0.76.9) (43.0.3)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai==0.76.9) (2.9.0)\n",
            "Requirement already satisfied: urllib3<3.0.0,>=2.0.7 in /usr/local/lib/python3.10/dist-packages (from auth0-python>=4.7.1->crewai==0.76.9) (2.2.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.12.3->crewai_tools==0.13.4) (2.6)\n",
            "Collecting build>=1.0.3 (from chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (0.20.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (4.66.6)\n",
            "Collecting overrides>=7.3.1 (from chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (1.67.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (0.13.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (3.10.11)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (0.27.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb>=0.4.24->crewai==0.76.9) (13.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.5)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.5)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting alembic<2.0.0,>=1.13.1 (from embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting cohere<6.0,>=5.3 (from embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading cohere-5.11.4-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.26.1 in /usr/local/lib/python3.10/dist-packages (from embedchain>=0.1.114->crewai_tools==0.13.4) (1.71.1)\n",
            "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading gptcache-0.1.44-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting langchain-cohere<0.4.0,>=0.3.0 (from embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading langchain_cohere-0.3.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting langchain-openai<0.3.0,>=0.2.1 (from embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading langchain_openai-0.2.8-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting mem0ai<0.2.0,>=0.1.29 (from embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading mem0ai-0.1.29-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pypdf<6.0.0,>=5.0.0 (from embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting pysbd<0.4.0,>=0.3.4 (from embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting schema<0.8.0,>=0.7.5 (from embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Collecting tiktoken<0.8.0,>=0.7.0 (from embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor>=1.3.3->crewai==0.76.9) (0.16)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from instructor>=1.3.3->crewai==0.76.9) (3.1.4)\n",
            "Collecting jiter<0.7,>=0.6.1 (from instructor>=1.3.3->crewai==0.76.9)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor>=1.3.3->crewai==0.76.9) (2.23.4)\n",
            "Collecting deprecation (from lancedb>=0.5.4->crewai_tools==0.13.4)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: nest-asyncio~=1.0 in /usr/local/lib/python3.10/dist-packages (from lancedb>=0.5.4->crewai_tools==0.13.4) (1.6.0)\n",
            "Collecting pylance==0.19.2 (from lancedb>=0.5.4->crewai_tools==0.13.4)\n",
            "  Downloading pylance-0.19.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lancedb>=0.5.4->crewai_tools==0.13.4) (24.2)\n",
            "Requirement already satisfied: pyarrow>=12 in /usr/local/lib/python3.10/dist-packages (from pylance==0.19.2->lancedb>=0.5.4->crewai_tools==0.13.4) (17.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.2.16->crewai==0.76.9) (0.3.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_community==0.3.5) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community==0.3.5) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.22->crewai==0.76.9) (8.5.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.22->crewai==0.76.9) (4.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.13.3->crewai==0.76.9) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.13.3->crewai==0.76.9) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.13.3->crewai==0.76.9) (1.3.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.22.0->crewai==0.76.9) (1.2.14)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai==0.76.9) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai==0.76.9)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai==0.76.9)\n",
            "  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.22.0 (from crewai==0.76.9)\n",
            "  Downloading opentelemetry_sdk-1.28.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf<6.0,>=5.0 (from opentelemetry-proto==1.28.2->opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai==0.76.9)\n",
            "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-api>=1.22.0 (from crewai==0.76.9)\n",
            "  Downloading opentelemetry_api-1.28.2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.49b2 (from opentelemetry-sdk>=1.22.0->crewai==0.76.9)\n",
            "  Downloading opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.4.2->crewai==0.76.9) (0.7.0)\n",
            "Collecting nodeenv>=1.6.0 (from pyright>=1.1.350->crewai_tools==0.13.4)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=8.0.0->crewai_tools==0.13.4) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest>=8.0.0->crewai_tools==0.13.4) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=8.0.0->crewai_tools==0.13.4) (1.2.2)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis>=0.3.2->crewai==0.76.9) (7.34.0)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis>=0.3.2->crewai==0.76.9) (4.0.0)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis>=0.3.2->crewai==0.76.9) (3.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->crewai_tools==0.13.4) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->crewai_tools==0.13.4) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->crewai_tools==0.13.4) (2024.8.30)\n",
            "Collecting trio~=0.17 (from selenium>=4.18.1->crewai_tools==0.13.4)\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium>=4.18.1->crewai_tools==0.13.4)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium>=4.18.1->crewai_tools==0.13.4) (1.8.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain_community==0.3.5) (3.1.1)\n",
            "Collecting Mako (from alembic<2.0.0,>=1.13.1->embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<44.0.0,>=43.0.1->auth0-python>=4.7.1->crewai==0.76.9) (1.17.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.22.0->crewai==0.76.9) (1.16.0)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (1.25.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (1.13.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (2.0.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from gptcache<0.2.0,>=0.1.43->embedchain>=0.1.114->crewai_tools==0.13.4) (5.5.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb>=0.4.24->crewai==0.76.9) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=0.4.24->crewai==0.76.9) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.44.22->crewai==0.76.9) (3.21.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor>=1.3.3->crewai==0.76.9) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_community==0.3.5) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai==0.76.9) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai==0.76.9) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai==0.76.9) (0.21.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai==0.76.9) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai==0.76.9) (2.8.2)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai==0.76.9) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai==0.76.9) (3.2.2)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting langchain-experimental>=0.3.0 (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading langchain_experimental-0.3.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pandas>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools==0.13.4) (2.2.2)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools==0.13.4) (0.9.0)\n",
            "Requirement already satisfied: pytz<2025.0,>=2024.1 in /usr/local/lib/python3.10/dist-packages (from mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4) (2024.2)\n",
            "Collecting qdrant-client<2.0.0,>=1.9.1 (from mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading qdrant_client-1.12.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.24->crewai==0.76.9) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.4.24->crewai==0.76.9) (1.13.1)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb>=0.4.24->crewai==0.76.9) (3.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb>=0.4.24->crewai==0.76.9) (0.26.2)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium>=4.18.1->crewai_tools==0.13.4)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting outcome (from trio~=0.17->selenium>=4.18.1->crewai_tools==0.13.4)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium>=4.18.1->crewai_tools==0.13.4)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb>=0.4.24->crewai==0.76.9) (1.5.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.5)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium>=4.18.1->crewai_tools==0.13.4) (1.7.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<44.0.0,>=43.0.1->auth0-python>=4.7.1->crewai==0.76.9) (2.22)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (1.62.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (2.7.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (0.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=0.4.24->crewai==0.76.9) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=0.4.24->crewai==0.76.9) (2024.10.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.4.24->crewai==0.76.9) (0.1.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools==0.13.4) (2024.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis>=0.3.2->crewai==0.76.9) (0.2.13)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading grpcio_tools-1.68.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.4.24->crewai==0.76.9) (1.3.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (1.6.0)\n",
            "Collecting grpcio>=1.58.0 (from chromadb>=0.4.24->crewai==0.76.9)\n",
            "  Downloading grpcio-1.68.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools==0.13.4) (0.6.1)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai_tools==0.13.4)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading crewai-0.76.9-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.2/191.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crewai_tools-0.13.4-py3-none-any.whl (463 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.7/463.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.5-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading auth0_python-4.7.2-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.8/131.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.5.18-py3-none-any.whl (615 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading embedchain-0.1.125-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.4/211.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading instructor-1.6.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.30.2-py3-none-any.whl (18 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading lancedb-0.16.0-cp38-abi3-manylinux_2_28_x86_64.whl (27.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/27.4 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pylance-0.19.2-cp39-abi3-manylinux_2_28_x86_64.whl (30.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading litellm-1.52.9-py3-none-any.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.28.2-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.28.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.28.2-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.8/118.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.28.2-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.2/159.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading pyright-1.1.389-py3-none-any.whl (18 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.1.0-py3-none-any.whl (6.4 kB)\n",
            "Downloading uv-0.5.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading cohere-5.11.4-py3-none-any.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.7/249.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gptcache-0.1.44-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_cohere-0.3.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mem0ai-0.1.29-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl (6.9 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.3.3-py3-none-any.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdrant_client-1.12.1-py3-none-any.whl (267 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.2/267.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading grpcio_tools-1.68.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.68.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: docx2txt, pypika\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=769a0fd5e4ec98ddc8bbac9489a7d5faa172b375ec9f87670a0ff634297929ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=cd396412ff8cd67bc4c33e1eabde6b7b2be013f35c5fd786b5e7745147a6ba14\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built docx2txt pypika\n",
            "Installing collected packages: sortedcontainers, schema, pypika, monotonic, durationpy, docx2txt, appdirs, wsproto, websockets, uvloop, uvicorn, uv, types-requests, tomli-w, SQLAlchemy, pytube, python-dotenv, pysbd, pyproject_hooks, pypdf, protobuf, portalocker, parameterized, overrides, outcome, opentelemetry-util-http, nodeenv, mypy-extensions, mmh3, marshmallow, Mako, jsonref, json-repair, jiter, jedi, hyperframe, humanfriendly, httpx-sse, httptools, hpack, grpcio, fastavro, deprecation, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, trio, tiktoken, starlette, pyright, pylance, posthog, opentelemetry-proto, opentelemetry-api, h2, grpcio-tools, gptcache, docker, coloredlogs, build, alembic, trio-websocket, pyvis, pydantic-settings, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, lancedb, kubernetes, fastapi, dataclasses-json, selenium, qdrant-client, opentelemetry-sdk, opentelemetry-instrumentation, litellm, instructor, cohere, auth0-python, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, mem0ai, langchain-openai, opentelemetry-instrumentation-fastapi, langchain_community, chromadb, langchain-experimental, langchain-cohere, embedchain, crewai_tools, crewai\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.7.1\n",
            "    Uninstalling jiter-0.7.1:\n",
            "      Successfully uninstalled jiter-0.7.1\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.67.1\n",
            "    Uninstalling grpcio-1.67.1:\n",
            "      Successfully uninstalled grpcio-1.67.1\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.28.1\n",
            "    Uninstalling opentelemetry-api-1.28.1:\n",
            "      Successfully uninstalled opentelemetry-api-1.28.1\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.49b1\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.49b1:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.49b1\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.28.1\n",
            "    Uninstalling opentelemetry-sdk-1.28.1:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.6 SQLAlchemy-2.0.35 alembic-1.14.0 appdirs-1.4.4 asgiref-3.8.1 auth0-python-4.7.2 backoff-2.2.1 bcrypt-4.2.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.18 cohere-5.11.4 coloredlogs-15.0.1 crewai-0.76.9 crewai_tools-0.13.4 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 docx2txt-0.8 durationpy-0.9 embedchain-0.1.125 fastapi-0.115.5 fastavro-1.9.7 gptcache-0.1.44 grpcio-1.68.0 grpcio-tools-1.68.0 h2-4.1.0 hpack-4.0.0 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 hyperframe-6.0.1 instructor-1.6.4 jedi-0.19.2 jiter-0.6.1 json-repair-0.30.2 jsonref-1.1.0 kubernetes-31.0.0 lancedb-0.16.0 langchain-cohere-0.3.1 langchain-experimental-0.3.3 langchain-openai-0.2.8 langchain_community-0.3.5 litellm-1.52.9 marshmallow-3.23.1 mem0ai-0.1.29 mmh3-5.0.1 monotonic-1.6 mypy-extensions-1.0.0 nodeenv-1.9.1 onnxruntime-1.20.0 opentelemetry-api-1.28.2 opentelemetry-exporter-otlp-proto-common-1.28.2 opentelemetry-exporter-otlp-proto-grpc-1.28.2 opentelemetry-exporter-otlp-proto-http-1.28.2 opentelemetry-instrumentation-0.49b2 opentelemetry-instrumentation-asgi-0.49b2 opentelemetry-instrumentation-fastapi-0.49b2 opentelemetry-proto-1.28.2 opentelemetry-sdk-1.28.2 opentelemetry-semantic-conventions-0.49b2 opentelemetry-util-http-0.49b2 outcome-1.3.0.post0 overrides-7.7.0 parameterized-0.9.0 portalocker-2.10.1 posthog-3.7.0 protobuf-5.28.3 pydantic-settings-2.6.1 pylance-0.19.2 pypdf-5.1.0 pypika-0.48.9 pyproject_hooks-1.2.0 pyright-1.1.389 pysbd-0.3.4 python-dotenv-1.0.1 pytube-15.0.0 pyvis-0.3.2 qdrant-client-1.12.1 schema-0.7.7 selenium-4.26.1 sortedcontainers-2.4.0 starlette-0.41.2 tiktoken-0.7.0 tomli-w-1.1.0 trio-0.27.0 trio-websocket-0.11.1 types-requests-2.32.0.20241016 typing-inspect-0.9.0 uv-0.5.2 uvicorn-0.32.0 uvloop-0.21.0 watchfiles-0.24.0 websockets-14.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install crewai==0.76.9 crewai_tools==0.13.4 langchain_community==0.3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "iP3Dq4cOBlpw"
      },
      "outputs": [],
      "source": [
        "#!pip install CrewAI crewai_tools langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "Y-XsXo9YCa-C"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltjVe7aCH_VR",
        "outputId": "f3dfb7bf-3ef9-4261-a641-6bf9702d5847",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.2.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-groq) (0.3.18)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-groq) (0.1.143)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-groq) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-groq) (9.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-groq) (3.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-groq) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-groq) (2.2.3)\n",
            "Downloading langchain_groq-0.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.12.0-py3-none-any.whl (108 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/108.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.12.0 langchain-groq-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EwZrSdPed_6M"
      },
      "outputs": [],
      "source": [
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cRlaH0k7zI-H",
        "outputId": "29336611-5a3f-4e71-add6-558a3f615ee3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.26.2)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.3.18)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (3.2.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.20.3)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.1.143)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (9.0.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.2.2)\n",
            "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: langchain_huggingface\n",
            "Successfully installed langchain_huggingface-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5buKQOIT9ZyO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools  import tool\n",
        "import os\n",
        "from crewai_tools import PDFSearchTool\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tGt6JOL1UdGr",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# os.environ[\"GROQ_API_KEY\"] = userdata.get('GROK_LLAMA3')\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"placeholder_key\"\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "serper_api_key = userdata.get('SERPER_API_KEY')\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_MODEL_NAME\"] = 'gpt-4o-mini'\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API_KEY')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools"
      ],
      "metadata": {
        "id": "VlCuaGP_2Hsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PDF Read Tool\n",
        "\n",
        "rag_tool = PDFSearchTool(\n",
        "    pdf='/content/Arabic_ RAG sampels Dataset.pdf',\n",
        "    config=dict(\n",
        "        llm=dict(\n",
        "            provider=\"openai\",  # Using OpenAI instead of Groq\n",
        "            config=dict(\n",
        "                model=os.environ.get(\"gpt-4o-mini\", \"gpt-4\"),  # Fetching model name from environment variable\n",
        "                temperature=0.5,  # Optional: Adjust for response variability\n",
        "                top_p=1,  # Optional: Adjust for nucleus sampling\n",
        "                stream=True,  # Optional: Enables streaming responses\n",
        "            ),\n",
        "        ),\n",
        "        embedder=dict(\n",
        "            provider=\"huggingface\",  # Keeping Hugging Face for embeddings\n",
        "            config=dict(\n",
        "                model=\"BAAI/bge-small-en-v1.5\",  # Specify the embedding model\n",
        "                # task_type=\"retrieval_document\",  # Optional, uncomment if required\n",
        "                # title=\"Embeddings\",  # Optional, uncomment if required\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "BUa-pjnEvMKQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TAVILY for internet sarch\n",
        "os.environ['TAVILY_API_KEY'] = userdata.get('TAVILY_API_KEY')\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ],
      "metadata": {
        "id": "2WwhEWY8vQzM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_tool.run(\"hi what this document about\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "mygWuvwnzahy",
        "outputId": "0527370a-8a95-43f5-ef02-0e2448f7330e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Tool: Search a PDF's content\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Relevant Content:\\nفترة التداول، وذلك حسب الإجراء الآتي: 1. يتقدم المصدر من خلال ممثليه المعينين أمام السوق بطلب تعليق التداول المؤقت لأوراقه المالية المدرجة قبل نصف ساعة على الأقل من وقت بداية التعليق الذي سيحدده المصدر، ويجب أن يحتوي الطلب على الآتي: ○ وقت بداية التعليق ○ مدة التعليق ○ مبررات طلب التعليق\\n\\nالمؤقت بناء على طلب المصدر 2. عند عدم التزام المصدر بالمواعيد المحددة للإفصاح عن معلوماته المالية الدورية وفق اللوائح التنفيذية ذات العلاقة 3. عند تضمن تقرير مراجع الحسابات على القوائم المالية للمصدر رأي معارض أو امتناع عن إبداء الرأي 4. عند صدور قرار عن الجمعية العامة غير العادية للمصدر بتخفيض رأس ماله وذلك ليومي التداول التاليين لصدور القرار ب. لا تخلّ هذه الإجراءات بالأحكام الواردة في نظام السوق المالية ولوائحه التنفيذية وقواعد السوق والأنظمة الأخرى ذات العلاقة. ج. لا يحول اتخاذ أي إجراء وارد في هذه الإجراءات دون إيقاع الجزاءات المقررة على المصدر في حال خالف أي من أحكام نظام السوق المالية ولوائحه التنفيذية وقواعد السوق. إجراءات تعليق تداول الأوراق المالية المدرجة أولاً: تعليق تداول الأوراق المالية المدرجة بناءً على طلب المصدر وفقاً للمادة الثامنة والثلاثين من قواعد الإدراج، يجوز للمصدر أن يطلب من السوق تعليق تداول أوراقه المالية مؤقتاً عند وقوع حدث خلال فترة التداول يجب الإفصاح عنه من دون تأخير بموجب النظام أو لوائحه التنفيذية أو قواعد السوق ولا يستطيع المصدر تأمين سريته حتى نهاية\\n\\nالعلاقة، تقوم السوق بتعليق تداول الأوراق المالية لجلسة تداول واحدة تلي انتهاء المهلة النظامية وتعلن عن سبب التعليق مع التوضيح في الإعلان بانطباق هذه الإجراءات على المصدر. 2. يجب على المصدر الإعلان عن عدم تمكنه من نشر معلوماته المالية الدورية قبل نهاية المهلة المحددة في اللوائح التنفيذية ذات العلاقة على موقع السوق المالية السعودية (تداول) مع التوضيح في الإعلان بانطباق هذه الإجراءات عليه. 3. يستأنف تداول الأوراق المالية ذات العلاقة لمدة عشرين جلسة تداول تلي الجلسة التي تم تعليق تداولها فيها، ويجب على المصدر خلال هذه المدة نشر معلوماته المالية الدورية. 4. في حال لم ينشر المصدر معلوماته المالية خلال المدة المشار إليها في الفقرة3) أعلاه، تقوم السوق بالإعلان عن إعادة تعليق الأوراق المالية، إلى أن يقوم المصدر بالإعلان عن نتائجه المالية الدورية. 5. تقوم السوق برفع التعليق بعد مرور جلسة تداول واحدة تلي الإعلان عن نتائجه المالية الدورية، وتقوم بالإعلان عن ذلك 6. في حال تجاوز تعليق الأوراق المالية شهراً واحداً فللمصدر التقدم بطلب تداول أوراقه المالية خارج المنصة 7. في حال استمر تعليق تداول'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "web_search_tool.run(\"what is Navid.sa\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJUxhPrzzeR0",
        "outputId": "0a63831a-6dcb-46cd-bb50-a4005705cc90"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://navid.sa/',\n",
              "  'content': 'Navid specializes in providing end-to-end data center solutions to cater to the burgeoning demands of AI and computational workloads. We understand the critical role that robust infrastructure plays in powering AI-driven applications, and thus, we are committed to delivering cutting-edge solutions. ... info@navid.sa +966 138870349;'},\n",
              " {'url': 'https://www.linkedin.com/in/navid-sa-3b329072',\n",
              "  'content': \"View navid sa's professional profile on LinkedIn. LinkedIn is the world's largest business network, helping professionals like navid sa discover inside connections to recommended job\"},\n",
              " {'url': 'https://navid.sa/about/',\n",
              "  'content': 'Our vision with Navid is to cultivate broad AI expertise that extends beyond foundational areas and ventures into cutting-edge fields. In domains like Natural Language Processing, Neural Network Architecture, Robotics, AI Hardware Infrastructure, AI Software Infrastructure, and Computer Vision, we aim to pioneer transformative applications and'},\n",
              " {'url': 'https://sa.linkedin.com/company/navidco',\n",
              "  'content': \"Navid | نفيد | ١٣٨ من المتابعين على LinkedIn. Empower the future of Ai | At Navid, we believe that a key differentiator in an intelligent organization is combining human power with advanced data management and automation technology. We focus on improving our client's profitability by creating artificial intelligence (AI) systems built to boost revenue, reduce operational\"},\n",
              " {'url': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/',\n",
              "  'content': 'AI agents build on this potential by accessing diverse data through accelerated AI query engines, which process, store and retrieve information to enhance generative AI models. The end-to-end NVIDIA AI platform, including NVIDIA NeMo microservices, provides the ability to manage and access data efficiently, which is crucial for building responsive agentic AI applications. To accelerate the adoption of generative AI-powered applications and agents, NVIDIA NIM Agent Blueprints provide sample applications, reference code, sample data, tools and comprehensive documentation. NVIDIA partners including Accenture are helping enterprises use agentic AI with solutions built with NIM Agent Blueprints. Visit ai.nvidia.com to learn more about the tools and software NVIDIA offers to help enterprises build their own AI agents. NVIDIA Works With Deloitte to Deploy Digital AI Agents for Healthcare'}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "web_search_tool.run(\"who is Hamza Shahid whose worked in Navid.SA\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9PcO_H31TCv",
        "outputId": "0fab8b46-67a3-43d7-872f-120c24e4b763"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://sa.linkedin.com/in/hamza-shahid-05613658',\n",
              "  'content': 'Hamza is a very sincere and dedicated employee, who works to a very high Safety and Quality standards. He always strives for the best level of safety and Quality in an extremely difficult working environment with great attention to detail. I found Hamza a very easy person to work with as he is very enthusiastic to take up new challenges.'},\n",
              " {'url': 'https://navid.sa/',\n",
              "  'content': \"At Navid, we're at the forefront of harnessing the power of Low Earth Orbit (LEO) satellites to revolutionize communication, monitoring, and data analysis. Our satellite technology opens up new horizons for global connectivity, detailed Earth observation, and advanced data analysis, all designed to meet the demands of tomorrow. How it work.\"},\n",
              " {'url': 'https://muckrack.com/hamza-shahid',\n",
              "  'content': 'Hamza Shahid Is this you? As a journalist, you can create a free Muck Rack account to customize your profile, list your contact preferences, and upload a portfolio of your best work. Claim your profile Get in touch with Hamza. Contact Hamza, search articles and Tweets, monitor coverage, and track replies from one place. Learn more about Muck'},\n",
              " {'url': 'https://www.linkedin.com/in/hamzashahid',\n",
              "  'content': 'Making Things Work San Francisco, CA. Connect Sean Dillon New York City Metropolitan Area ... Others named Hamza Shahid in United States. Hamza Shahid Edison, NJ. Hamza Shahid'},\n",
              " {'url': 'https://navid.sa/about/',\n",
              "  'content': 'Our Vision. Our vision with Navid is to cultivate broad AI expertise that extends beyond foundational areas and ventures into cutting-edge fields. In domains like Natural Language Processing, Neural Network Architecture, Robotics, AI Hardware Infrastructure, AI Software Infrastructure, and Computer Vision, we aim to pioneer transformative'}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "web_search_tool.run(\"what is Mullhem that  in Navid.SA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl9trnku1orV",
        "outputId": "d0f6e545-18f1-48f7-fcad-ef74cdaa86a4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'http://navid.sa/mulhem/',\n",
              "  'content': 'Mulhem. Navid is proud to announce the launch of Mulhem, a groundbreaking artificial intelligence model that marks a significant milestone in the field of AI within the Kingdom of Saudi Arabia.'},\n",
              " {'url': 'https://www.linkedin.com/posts/navidco_ملهم-نفيد-mulhem-activity-7247609183062085632-jltI',\n",
              "  'content': 'For more information: 🌐 navid.sa/contact #LLM #AI #ArtificialIntelligence #AIForBusiness #GenerativeAI #NLP ... Mulheim Rag offers unique features like bilingual capabilities, cultural'},\n",
              " {'url': 'https://navid.sa/',\n",
              "  'content': 'Navid, AI & Infrastructure Solution Provider. Examine the Potential of Infrastructure Connectivity Intelligence Performance Innovation Resilience Security Strategy Insight Ethics With Navid . At Navid, we believe that a key differentiator in an intelligent organization is combining human power with advanced data management and automation technology.'},\n",
              " {'url': 'https://navid.sa/about/',\n",
              "  'content': 'Our Vision Our vision with Navid is to cultivate broad AI expertise that extends beyond foundational areas and ventures into cutting-edge fields.'},\n",
              " {'url': 'https://www.fool.com/investing/2024/05/14/3-nvidia-partners-with-explosive-growth-potential/',\n",
              "  'content': 'Nvidia (NASDAQ: NVDA) embodies artificial intelligence (AI) enthusiasm unlike any other growth stock. But what if you already own enough Nvidia, or are simply looking for other opportunities?'}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def router_tool(question):\n",
        "  \"\"\"Router Function\"\"\"\n",
        "  if 'Sporo Health' in question:\n",
        "    return 'vectorstore'\n",
        "  else:\n",
        "    return 'web_search'"
      ],
      "metadata": {
        "id": "69XSvhn713yy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents"
      ],
      "metadata": {
        "id": "ESpVwSj71_wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Router_Agent\n",
        "\n",
        "Router_Agent = Agent(\n",
        "  role='Router',\n",
        "  goal='Route user question to a vectorstore or web search',\n",
        "  backstory=(\n",
        "    \"You are an expert at routing a user question to a vectorstore or web search.\"\n",
        "    \"Use the vectorstore for questions on concept related to Retrieval-Augmented Generation.\"\n",
        "    \"You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web-search.\"\n",
        "  ),\n",
        "  verbose=True,\n",
        "  allow_delegation=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "m9UUC9WC204a"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Retriever Agent\n",
        "Retriever_Agent = Agent(\n",
        "role=\"Retriever\",\n",
        "goal=\"Use the information retrieved from the vectorstore to answer the question\",\n",
        "backstory=(\n",
        "    \"You are an assistant for question-answering tasks.\"\n",
        "    \"Use the information present in the retrieved context to answer the question.\"\n",
        "    \"You have to provide a clear concise answer.\"\n",
        "),\n",
        "verbose=True,\n",
        "allow_delegation=False\n",
        ")"
      ],
      "metadata": {
        "id": "W4GC_fs12-EX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Grader agent\n",
        "Grader_agent =  Agent(\n",
        "  role='Answer Grader',\n",
        "  goal='Filter out erroneous retrievals',\n",
        "  backstory=(\n",
        "    \"You are a grader assessing relevance of a retrieved document to a user question.\"\n",
        "    \"If the document contains keywords related to the user question, grade it as relevant.\"\n",
        "    \"It does not need to be a stringent test.You have to make sure that the answer is relevant to the question.\"\n",
        "  ),\n",
        "  verbose=True,\n",
        "  allow_delegation=False\n",
        ")"
      ],
      "metadata": {
        "id": "8JnT9A6S3Ihv"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Hallucination grader\n",
        "hallucination_grader = Agent(\n",
        "    role=\"Hallucination Grader\",\n",
        "    goal=\"Filter out hallucination\",\n",
        "    backstory=(\n",
        "        \"You are a hallucination grader assessing whether an answer is grounded in / supported by a set of facts.\"\n",
        "        \"Make sure you meticulously review the answer and check if the response provided is in alignmnet with the question asked\"\n",
        "    ),\n",
        "    verbose=True,\n",
        "    allow_delegation=False)\n"
      ],
      "metadata": {
        "id": "0FKbcokI3Rxa"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer Grader\n",
        "answer_grader = Agent(\n",
        "    role=\"Answer Grader\",\n",
        "    goal=\"Filter out hallucination from the answer.\",\n",
        "    backstory=(\n",
        "        \"You are a grader assessing whether an answer is useful to resolve a question.\"\n",
        "        \"Make sure you meticulously review the answer and check if it makes sense for the question asked\"\n",
        "        \"If the answer is relevant generate a clear and concise response.\"\n",
        "        \"If the answer gnerated is not relevant then perform a websearch using 'web_search_tool'\"\n",
        "    ),\n",
        "    verbose=True,\n",
        "    allow_delegation=False\n",
        ")"
      ],
      "metadata": {
        "id": "qCDlNzxW3agS"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_MLLsQoP3n3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks"
      ],
      "metadata": {
        "id": "vTQ2jf8v6jqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Router task\n",
        "router_task = Task(\n",
        "    description=(\"Analyse the keywords in the question {question}\"\n",
        "    \"Based on the keywords decide whether it is eligible for a vectorstore search or a web search.\"\n",
        "    \"Return a single word 'vectorstore' if it is eligible for vectorstore search.\"\n",
        "    \"Return a single word 'websearch' if it is eligible for web search.\"\n",
        "    \"Do not provide any other premable or explaination.\"\n",
        "    ),\n",
        "    expected_output=(\"Give a binary choice 'websearch' or 'vectorstore' based on the question\"\n",
        "    \"Do not provide any other premable or explaination.\"),\n",
        "    agent=Router_Agent,\n",
        "    tools=[router_tool],\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TcCtK9K_6mZ_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Retriever Task\n",
        "\n",
        "retriever_task = Task(\n",
        "    description=(\"Based on the response from the router task extract information for the question {question} with the help of the respective tool.\"\n",
        "    \"Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.\"\n",
        "    \"Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\"\n",
        "    ),\n",
        "    expected_output=(\"You should analyse the output of the 'router_task'\"\n",
        "    \"If the response is 'websearch' then use the web_search_tool to retrieve information from the web.\"\n",
        "    \"If the response is 'vectorstore' then use the rag_tool to retrieve information from the vectorstore.\"\n",
        "    \"Return a claer and consise text as response.\"),\n",
        "    agent=Retriever_Agent,\n",
        "    context=[router_task],\n",
        "  #  tools=[retriever_tool],\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9GJQ6ibO6sPh"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Grader Task\n",
        "grader_task = Task(\n",
        "    description=(\"Based on the response from the retriever task for the quetion {question} evaluate whether the retrieved content is relevant to the question.\"\n",
        "    ),\n",
        "    expected_output=(\"Binary score 'yes' or 'no' score to indicate whether the document is relevant to the question\"\n",
        "    \"You must answer 'yes' if the response from the 'retriever_task' is in alignment with the question asked.\"\n",
        "    \"You must answer 'no' if the response from the 'retriever_task' is not in alignment with the question asked.\"\n",
        "    \"Do not provide any preamble or explanations except for 'yes' or 'no'.\"),\n",
        "    agent=Grader_agent,\n",
        "    context=[retriever_task],\n",
        ")"
      ],
      "metadata": {
        "id": "Zzm0AIp76yZ4"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Hallucination_task\n",
        "\n",
        "hallucination_task = Task(\n",
        "    description=(\"Based on the response from the grader task for the quetion {question} evaluate whether the answer is grounded in / supported by a set of facts.\"),\n",
        "    expected_output=(\"Binary score 'yes' or 'no' score to indicate whether the answer is sync with the question asked\"\n",
        "    \"Respond 'yes' if the answer is in useful and contains fact about the question asked.\"\n",
        "    \"Respond 'no' if the answer is not useful and does not contains fact about the question asked.\"\n",
        "    \"Do not provide any preamble or explanations except for 'yes' or 'no'.\"),\n",
        "    agent=hallucination_grader,\n",
        "    context=[grader_task],\n",
        ")\n"
      ],
      "metadata": {
        "id": "_LZb66lQ7Byl"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer Task\n",
        "\n",
        "answer_task = Task(\n",
        "    description=(\"Based on the response from the hallucination task for the quetion {question} evaluate whether the answer is useful to resolve the question.\"\n",
        "    \"If the answer is 'yes' return a clear and concise answer.\"\n",
        "    \"If the answer is 'no' then perform a 'websearch' and return the response\"),\n",
        "    expected_output=(\"Return a clear and concise response if the response from 'hallucination_task' is 'yes'.\"\n",
        "    \"Perform a web search using 'web_search_tool' and return ta clear and concise response only if the response from 'hallucination_task' is 'no'.\"\n",
        "    \"Otherwise respond as 'Sorry! unable to find a valid response'.\"),\n",
        "    context=[hallucination_task],\n",
        "    agent=answer_grader,\n",
        "    #tools=[answer_grader_tool],\n",
        ")"
      ],
      "metadata": {
        "id": "_Ueo7-vo7LhP"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crew"
      ],
      "metadata": {
        "id": "2u3GNkRd9klS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_crew = Crew(\n",
        "    agents=[Router_Agent, Retriever_Agent, Grader_agent, hallucination_grader, answer_grader],\n",
        "    tasks=[router_task, retriever_task, grader_task, hallucination_task, answer_task],\n",
        "    verbose=True,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "EUG6-0397Swr"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Input\n",
        "inputs ={\"question\":\"في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، ما هي الإجراءات التي تتبعها السوق المالية السعودية (تداول)؟ واذكر بالتفصيل التوقيتات المحددة لكل خطوة.\"}\n"
      ],
      "metadata": {
        "id": "toa7yjEY7Uwr"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Result\n",
        "result = rag_crew.kickoff(inputs=inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1TNSWIL7uxM",
        "outputId": "4ab9a7ca-cb61-484a-9c16-041c5426d3f8"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRouter\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyse the keywords in the question في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، ما هي الإجراءات التي تتبعها السوق المالية السعودية (تداول)؟ واذكر بالتفصيل التوقيتات المحددة لكل خطوة.Based on the keywords decide whether it is eligible for a vectorstore search or a web search.Return a single word 'vectorstore' if it is eligible for vectorstore search.Return a single word 'websearch' if it is eligible for web search.Do not provide any other premable or explaination.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRouter\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "websearch\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRetriever\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the response from the router task extract information for the question في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، ما هي الإجراءات التي تتبعها السوق المالية السعودية (تداول)؟ واذكر بالتفصيل التوقيتات المحددة لكل خطوة. with the help of the respective tool.Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRetriever\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، تتبع السوق المالية السعودية (تداول) الإجراءات التالية:\n",
            "\n",
            "1. **إشعار المصدر**: يتم إرسال إشعار للمصدر يطلب فيه توضيحات حول عدم الالتزام بالموعد النهائي.\n",
            "2. **مهلة إضافية**: يتم منح المصدر مهلة إضافية لنشر المعلومات المطلوبة، وعادة ما تكون هذه المهلة بين 3 إلى 5 أيام عمل.\n",
            "3. **إجراءات تأديبية**: في حال عدم الامتثال بعد انتهاء المهلة الإضافية، تتخذ السوق المالية إجراءات تأديبية ضد المصدر، والتي قد تشمل فرض غرامة أو تعليق تداول أسهم المصدر.\n",
            "4. **إعلان عن المخالفات**: تُعلن السوق المالية عن المخالفات التي ارتكبها المصدر، مما قد يؤثر على سمعته في السوق.\n",
            "\n",
            "توقيتات هذه الإجراءات قد تختلف بناءً على الحالة المحددة، ولكنها تتبع هذه الخطوات بشكل عام.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAnswer Grader\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the response from the retriever task for the quetion في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، ما هي الإجراءات التي تتبعها السوق المالية السعودية (تداول)؟ واذكر بالتفصيل التوقيتات المحددة لكل خطوة. evaluate whether the retrieved content is relevant to the question.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAnswer Grader\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "yes\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mHallucination Grader\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the response from the grader task for the quetion في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، ما هي الإجراءات التي تتبعها السوق المالية السعودية (تداول)؟ واذكر بالتفصيل التوقيتات المحددة لكل خطوة. evaluate whether the answer is grounded in / supported by a set of facts.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mHallucination Grader\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "yes\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAnswer Grader\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the response from the hallucination task for the quetion في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، ما هي الإجراءات التي تتبعها السوق المالية السعودية (تداول)؟ واذكر بالتفصيل التوقيتات المحددة لكل خطوة. evaluate whether the answer is useful to resolve the question.If the answer is 'yes' return a clear and concise answer.If the answer is 'no' then perform a 'websearch' and return the response\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAnswer Grader\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، تتبع السوق المالية السعودية (تداول) الإجراءات التالية:\n",
            "\n",
            "1. **التنبيه الأول**: يبدأ بإشعار المصدر بعدم تقديم المعلومات في الوقت المحدد، ويكون عادة خلال يومين من تاريخ انتهاء المهلة.\n",
            "\n",
            "2. **فرصة لاستكمال المعلومات**: يتم منح المصدر فترة إضافية عادة لا تتجاوز 5 أيام عمل لتقديم المعلومات المطلوبة.\n",
            "\n",
            "3. **التنبيه الثاني**: إذا لم يتم تقديم المعلومات في الفترة الإضافية، يتم إصدار تنبيه ثاني، يحذر فيه المصدر من العواقب المحتملة.\n",
            "\n",
            "4. **وقف التداول**: في حال استمر عدم الامتثال، يجوز لسوق المال (تداول) اتخاذ قرار بوقف تداول أسهم الشركة حتى تقديم المعلومات اللازمة.\n",
            "\n",
            "5. **تقديم تقرير للجهات المختصة**: في حالات معينة، يُمكن أن تُرفع الحالة إلى الهيئة العامة للسوق المالية لأخذ الإجراءات القانونية المناسبة.\n",
            "\n",
            "تتعلق التوقيتات المحددة لكل خطوة بفترة المساءلة التي تتراوح بين يومين إلى 5 أيام، بالإضافة إلى فترة وقف التداول التي تكون وفق تقدير السوق.\u001b[00m\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"inputs\": {\n",
        "    \"question\": \"ما هو الحد الأقصى لمدة تعليق تداول الأوراق المالية إذا لم ينشر المصدر معلوماته المالية بعد انتهاء المهلة المحددة؟\"\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "result = rag_crew.kickoff(inputs=inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czAv4qmi7xqZ",
        "outputId": "da5af4b8-58e9-44f0-f8f1-99120bddff5e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRouter\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyse the keywords in the question في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، ما هي الإجراءات التي تتبعها السوق المالية السعودية (تداول)؟ واذكر بالتفصيل التوقيتات المحددة لكل خطوة.Based on the keywords decide whether it is eligible for a vectorstore search or a web search.Return a single word 'vectorstore' if it is eligible for vectorstore search.Return a single word 'websearch' if it is eligible for web search.Do not provide any other premable or explaination.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRouter\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "websearch\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRetriever\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the response from the router task extract information for the question في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، ما هي الإجراءات التي تتبعها السوق المالية السعودية (تداول)؟ واذكر بالتفصيل التوقيتات المحددة لكل خطوة. with the help of the respective tool.Use the web_serach_tool to retrieve information from the web in case the router task output is 'websearch'.Use the rag_tool to retrieve information from the vectorstore in case the router task output is 'vectorstore'.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRetriever\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "According to the regulations set by the Saudi Stock Exchange (Tadawul), if a listed company fails to publish its periodic financial information within the specified deadline, several procedures are followed:\n",
            "\n",
            "1. **Notification**: Tadawul will issue a notification to the company to remind it of its obligations and the importance of compliance.\n",
            "   \n",
            "2. **Public Disclosure**: The failure will be disclosed publicly through Tadawul’s official channels, which could entail the company being labeled as non-compliant on the exchange.\n",
            "\n",
            "3. **Penalties**: If the delay continues, Tadawul may impose penalties based on the severity of the breach. This could include fines or other disciplinary actions defined by the market rules.\n",
            "\n",
            "4. **Suspension**: If the lapse in compliance is not rectified, Tadawul might suspend trading of the company's shares until the necessary information is published.\n",
            "\n",
            "5. **Continuous Monitoring**: Tadawul will continue to monitor the situation and engage with the company for updates until the required financial reports are received.\n",
            "\n",
            "Each of these steps is clearly defined in Tadawul's regulatory framework, ensuring the market operates transparently and efficiently while protecting investors. Specific timing can vary, but typically notification and disclosure happen immediately following a missed deadline, with penalties and possible suspension being determined in subsequent days.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAnswer Grader\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the response from the retriever task for the quetion في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، ما هي الإجراءات التي تتبعها السوق المالية السعودية (تداول)؟ واذكر بالتفصيل التوقيتات المحددة لكل خطوة. evaluate whether the retrieved content is relevant to the question.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAnswer Grader\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "yes\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mHallucination Grader\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the response from the grader task for the quetion في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، ما هي الإجراءات التي تتبعها السوق المالية السعودية (تداول)؟ واذكر بالتفصيل التوقيتات المحددة لكل خطوة. evaluate whether the answer is grounded in / supported by a set of facts.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mHallucination Grader\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "yes\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAnswer Grader\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mBased on the response from the hallucination task for the quetion في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، ما هي الإجراءات التي تتبعها السوق المالية السعودية (تداول)؟ واذكر بالتفصيل التوقيتات المحددة لكل خطوة. evaluate whether the answer is useful to resolve the question.If the answer is 'yes' return a clear and concise answer.If the answer is 'no' then perform a 'websearch' and return the response\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAnswer Grader\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، تتبع السوق المالية السعودية (تداول) الإجراءات التالية:\n",
            "\n",
            "1. **الإخطار الأول**: يقوم تداول بإرسال إخطار رسمي إلى المصدر يفيد بعدم تقديم المعلومات في الوقت المحدد. يتضمن هذا الإخطار مهلة إضافية لتقديم المعلومات (عادةً ما تكون أسبوعاً واحداً).\n",
            "  \n",
            "2. **الإخطار الثاني**: إذا لم يتم استلام المعلومات بعد انتهاء المهلة الإضافية، يتم إرسال إخطار ثانٍ للمصدر يذكر أن عدم الامتثال قد يؤدي إلى فرض عقوبات.\n",
            "\n",
            "3. **الاجتماع مع المصدر**: بعد الإخطار الثاني، قد يتطلب تداول اجتماعاً مع إدارة المصدر لمناقشة أسباب التأخير وحل المشكلة.\n",
            "\n",
            "4. **الإجراءات القانونية أو العقوبات**: إذا استمر عدم الامتثال، يمكن أن تفرض السوق المالية عقوبات على المصدر، مثل تغريمهم أو إيقاف إدراج الأسهم.\n",
            "\n",
            "التوقيتات محددة كما يلي:\n",
            "- مدة الإخطار الأول: أسبوع من تاريخ انتهاء الفترة المحددة.\n",
            "- مدة الإخطار الثاني: يشمل مهلة إضافية تصل إلى أسبوع بعد الإخطار الأول.\n",
            "- إمكانية اجتماع مع إدارة المصدر: تُعقد حسب تطور الحالة بعد الإخطار الثاني.\n",
            "- فرض العقوبات: يبدأ اعتبارًا من تاريخ عدم الامتثال بعد الإجراءات السالفة.\u001b[00m\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsRq2hPr8W-4",
        "outputId": "6eef85d2-3b66-4c59-9d17-c962a23c2669"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "في حالة عدم تمكن المصدر من نشر معلوماته المالية الدورية خلال المهلة المحددة في اللوائح التنفيذية، تتبع السوق المالية السعودية (تداول) الإجراءات التالية:\n",
            "\n",
            "1. **الإخطار الأول**: يقوم تداول بإرسال إخطار رسمي إلى المصدر يفيد بعدم تقديم المعلومات في الوقت المحدد. يتضمن هذا الإخطار مهلة إضافية لتقديم المعلومات (عادةً ما تكون أسبوعاً واحداً).\n",
            "  \n",
            "2. **الإخطار الثاني**: إذا لم يتم استلام المعلومات بعد انتهاء المهلة الإضافية، يتم إرسال إخطار ثانٍ للمصدر يذكر أن عدم الامتثال قد يؤدي إلى فرض عقوبات.\n",
            "\n",
            "3. **الاجتماع مع المصدر**: بعد الإخطار الثاني، قد يتطلب تداول اجتماعاً مع إدارة المصدر لمناقشة أسباب التأخير وحل المشكلة.\n",
            "\n",
            "4. **الإجراءات القانونية أو العقوبات**: إذا استمر عدم الامتثال، يمكن أن تفرض السوق المالية عقوبات على المصدر، مثل تغريمهم أو إيقاف إدراج الأسهم.\n",
            "\n",
            "التوقيتات محددة كما يلي:\n",
            "- مدة الإخطار الأول: أسبوع من تاريخ انتهاء الفترة المحددة.\n",
            "- مدة الإخطار الثاني: يشمل مهلة إضافية تصل إلى أسبوع بعد الإخطار الأول.\n",
            "- إمكانية اجتماع مع إدارة المصدر: تُعقد حسب تطور الحالة بعد الإخطار الثاني.\n",
            "- فرض العقوبات: يبدأ اعتبارًا من تاريخ عدم الامتثال بعد الإجراءات السالفة.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zR_Jd4eN873V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}